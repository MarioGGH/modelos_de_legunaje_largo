# ğŸ§  Modelos de Lenguaje Local y API GroqCloud

Este proyecto demuestra cÃ³mo ejecutar modelos de lenguaje localmente con [Ollama](https://ollama.com) y cÃ³mo interactuar con la API de GroqCloud para generar respuestas a partir de prompts personalizados.

---

## ğŸ§© 1. InstalaciÃ³n de Ollama

Instala Ollama ejecutando el siguiente comando:

```sh
curl -fsSL https://ollama.com/install.sh | sh
```

---

## ğŸš€ 2. Iniciar el servicio de Ollama

```sh
ollama serve
```

---

## ğŸ“¦ 3. Descargar el modelo `gemma3:1b`

```sh
ollama pull gemma3:1b
```

---

## ğŸ’¬ 4. Ejecutar un prompt con el modelo `gemma3:1b`

```sh
ollama run gemma3:1b "Â¿De quÃ© color es el cielo?"
```

---

## ğŸŒ 5. Usar la API local de Ollama

Puedes generar respuestas directamente desde la API local:

```sh
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3:1b",
  "prompt": "Â¿De quÃ© color es el cielo?"
}'
```

### ğŸ¯ Con formato JSON (sin streaming):

```sh
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3:1b",
  "prompt": "Hola",
  "stream": false
}'
```

---

## ğŸ§ª 6. IntegraciÃ³n en una pÃ¡gina web

Crea un formulario en `index.html` para enviar prompts y mostrar respuestas:

```html
<div class="form">
  <form method="POST">
    <textarea name="response" id="response" rows="30" cols="50" readonly>$response</textarea><br>
    <label for="inp_prompt">Prompt</label><br>
    <input type="text" name="inp_prompt" id="inp_prompt" placeholder="Escribe tu prompt" required><br>
    <button type="submit" name="generar">Generar</button>
  </form>
</div>
```

Tu archivo `app.py` recibirÃ¡ el prompt desde el formulario:

```python
prompt = formulario.inp_prompt
```

Se construye el diccionario para la peticiÃ³n a la API:

```python
data = {
    "model": "gemma3:1b",
    "prompt": prompt,
    "stream": False
}
```

Se hace la peticiÃ³n a Ollama:

```python
url = "http://localhost:11434/api/generate"
response = requests.post(url, json=data)
response = json.loads(response.text)
```

Se renderiza la respuesta en la pÃ¡gina web:

```python
return render.index(response=response["response"])
```

---

## â˜ï¸ Uso de la API de GroqCloud

TambiÃ©n puedes usar la API de GroqCloud para generar respuestas desde la nube.

### ğŸ” Requiere API Key (reemplaza `{YOUR_API_KEY}`)

```sh
curl "https://api.groq.com/openai/v1/chat/completions" \
  -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer {YOUR_API_KEY}" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "1 + 1"
      }
    ],
    "model": "meta-llama/llama-4-scout-17b-16e-instruct",
    "temperature": 1,
    "max_completion_tokens": 1024,
    "top_p": 1,
    "stream": false
  }'
```

---

## ğŸ 1. Uso desde Python

Define los headers y el cuerpo de la peticiÃ³n:

```python
headers = {
    "Authorization": "Bearer {YOUR_API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "messages": [
        {
            "role": "user",
            "content": prompt
        }
    ],
    "model": "meta-llama/llama-4-scout-17b-16e-instruct",
    "stream": False,
    "temperature": 1,
    "stop": None
}

url = "https://api.groq.com/openai/v1/chat/completions"
```

---

## ğŸ“¡ 2. Hacer la peticiÃ³n

```python
response = requests.post(url, headers=headers, json=data)
response = json.loads(response.text)
```

---

## ğŸ’¾ 3. Obtener y mostrar el mensaje

```python
print(response["choices"][0]["message"]["content"])
response = response["choices"][0]["message"]["content"]
```

---

## ğŸ”„ 4. Retornar la respuesta en el HTML

```python
return render.index(response)
```

---

## âœ… ConclusiÃ³n

Este proyecto demuestra cÃ³mo:

* Ejecutar modelos de lenguaje de forma local con **Ollama**.
* Crear una **interfaz web simple** para interactuar con los modelos.
* Conectarse a la **API de GroqCloud** para generar texto desde la nube.
* Integrar fÃ¡cilmente estas tecnologÃ­as en tus aplicaciones web.
